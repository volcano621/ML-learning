<<<<<<< HEAD
# 逻辑回归 Logistic Regression

机器学习中用于解决二分问题的算法。

例如：

Kaggle中的Titanic数据集中，我们要实现根据Pclass、Age等属性来判断Survived是1还是0。先用训练集得到各属性和Survived之间的相关性（即weights），再用weights去predict测试集中每个人是否Survived

[img](image-20230919183711161.png)

前置知识：学习率

学习率（Learning Rate）是深度学习和机器学习中的一个重要超参数，它决定了模型在训练过程中参数更新的步长或速度。学习率的选择对模型的性能和训练效率具有重要影响，因此它是需要仔细调整的关键超参数之一。

## 传统方法

首先，**构建一个函数模型**（如Sigmoid函数），用这个函数表示从x到y的映射关系。

Sigmoid函数的公式如下：
$$
\hat{y} = \frac{1}{1 + e^{-z}}
$$
$\hat{y}$为预测的概率, $z=w^T*x$,x是输入特征向量，包含了每个特征的值,z是一个实数，表示线性组合的结果, w是权重。

因为这个函数的值域是(0,1)，所以符合我们对概率的要求

然后构建一个**损失函数loss function**。它描述了模型函数f ( x )和真实值y之间的差距。当然，这个差距越小越好。

似然函数：
$$
L = \hat{y}^y*(1-\hat{y})^{1-y}
$$


对数似然损失函数：
$$
J=y\log{\hat{y}}+(1−y)\log{(1-\hat{y})}
$$
首先计算损失函数对权重的导数：
$$
\frac{∂J}{∂w}=(\hat{y}−y)∗x
$$

可以说明损失函数随权重是变大还是变小的。如果损失函数随着权重的变大而变小，则说明权重应该变大。如果损失函数随着权重的变小而变小，则说明权重应该变小。所以权重的更新公式为：
$$
w = w - (\hat{y} - y) *x
$$
这样就能让误差最小

完整代码: 


```python
def logistic_regression(x, y, alpha, num_iterations):
    weights = np.ones((x.shape[1], 1))
    for i in range(num_iterations):
        z = np.dot(x, weights)
        h = sigmoid(z)
        error = h - y
        gradient = np.dot(x.T, error)
        weights -= alpha * gradient
    return weights
```



## SGD优化

与传统的梯度下降不同，SGD的关键特点之一是它对训练数据的采样具有随机性。在每次迭代中，SGD从训练数据集中随机选择一个小批量（mini-batch）的样本来计算梯度。这个小批量通常包含几十到几百个样本，具体大小取决于问题和计算资源。因此，SGD的每次迭代都只使用一部分数据来估计梯度，这样可以加速训练过程并帮助模型更快地收敛



## Adagrad优化

一种自适应学习率优化算法，旨在解决传统梯度下降法中学习率固定的问题。在传统的梯度下降算法中，学习率是一个常数，对所有模型参数都是一样的。这种情况下，如果某些参数的梯度较大，那么更新步长也会较大，可能导致参数在这些维度上更新得太快，甚至发生震荡，而其他参数的学习速度可能太慢，导致收敛速度较慢。

AdaGrad 的出现就是为了解决这个问题，它自适应地为每个参数分配不同的学习率。AdaGrad的核心思想是根据每个参数的历史梯度信息来调整学习率。通过累积每个参数的梯度平方和（$G_{i,i}$），算法能够了解每个参数在训练过程中梯度的大小和变化趋势。如果某个参数的梯度一直很大，那么它的学习率会逐渐减小，从而减小更新步长，防止参数在该维度上跳跃过大。相反，如果某个参数的梯度较小，它的学习率会逐渐增大，从而加快参数在该维度上的更新速度。这样，不同参数的学习率会根据其个体的梯度情况自动调整，实现了自适应性。

1. 初始化：选择初始参数 $$ \theta $$（就是之前说的w权重），初始化一个累积平方梯度的变量 $G$，初始化学习率 $\alpha$ 和一个小的常数 $\epsilon$ 以防止除零错误。

2. 对每个训练批次（batch）执行以下步骤：

   - 计算当前批次的梯度：$$g$$。

   - 累积平方梯度：$$G+=g^2$$。

   - 更新参数：对于每个参数 $\theta_i$，使用以下公式来更新它的值：
     $$
     \theta_i-= \frac{\alpha}{\epsilon + \sqrt{G}} *g
     $$



## RMSProp优化

RMSProp算法和Adagrad算法的区别在于 的计算由累积方式变成了指数加权移动平均值。AdaGrad 根据所有历史梯度对学习率进行衰减，这可能导致学习率在达到全局最优所在的凸结构前就变得太小了。RMSProp 使用指数加权移动平均值来削弱遥远过去的梯度的影响

1. 计算梯度：$g$

2. 更新累积平方梯度变量 $r$ 的指数加权平均：

   $$
   r = \rho r + (1 - \rho) g^2
   $$
   $\rho$是个常量，一般为0.9

3. 更新参数的公式：
   $$
   \theta_i  -= \frac{\alpha}{\sqrt{\epsilon+r}}* g
   $$

## AdaDelta

AdaDelta 算法在 RMSprop 算法的基础上将初始学习率改为动态计算的，针对的问题是：Adagrad算法需要自己手动指定初始学习率，而且由于分母中对历史梯度一直累加，学习率将逐渐下降至0，并且如果初始梯度很大的话，会导致整个训练过程的学习率一直很小，从而导致学习时间变长

1. $\Delta X$ 表示参数值变化的平均（即可表示梯度*步长），初始化为零，主要目的是确保在算法的开始阶段，$\Delta X$ 不会对参数的更新产生影响，因为在初始阶段，还没有足够的历史梯度信息可供使用。只有在经过一些迭代后，$\Delta X$ 才会逐渐积累梯度信息，从而影响参数的学习率。

2. 计算当前批次的梯度：$$g$$

3. 计算更新的平均平方梯度：$$r = \rho r + (1 - \rho) g^2$$

4. 计算参数更新：
   $$
   \Delta{\theta} =- \frac{\sqrt{\Delta{X}^2 + \epsilon}}{\sqrt{\epsilon+r}}
   $$

5. 更新参数：$$\theta -=\Delta{\theta}$$

6. 累计更新量：$$\Delta{X}^2 = \rho \Delta{X}^2 + (1 - \rho) (\Delta{\theta})^2$$

## 牛顿法

在牛顿法中，我们关心的是找到一个使得梯度为零的点（这样weights就是最合适的了）。牛顿法通常用于求解非线性方程或者用于找到某个函数的临界点，这个函数的梯度为零的地

假设我们现在要求方程f ‘( x ) = 0 的根x，先取一个初始点，作出过该点的切线，得到与x轴的交点（x1，0），再选取（x1,f（x1）)重复上述步骤，得到根x

可以求得$$x_{k+1}=x_k−\frac{f'(x0)}{f''(x0)}$$

$$f'(x_0)$$即为梯度，$$f''(x_0)$$可以用hessian矩阵表示
$$
H = 
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2} \\
\end{bmatrix}
$$


hessian矩阵近似为$$\sum_{i=0}^N b_nb_n^T$$（外积近似）

即

```python
hessian = np.dot(x.T, x) / m
weights -= np.dot(np.linalg.inv(hessian), gradient)  # 求逆
```



## Adam优化

RMSProp的优化，加入矫正偏差

1. 计算梯度：首先，计算当前迭代的梯度 $g_t$。

2. 更新一阶矩估计 $m_t$ 和二阶矩估计 $v_t$：
   $$
   m_t = β1 * m_{t-1} + (1 - β1) * g_t
   $$

   $$
   v_t = β2 * v_{t-1} + (1 - β2) * (g_t^2)
   $$

   

   这里，$m_{t-1}$ 和 $v_{t-1}$ 表示上一次迭代的一阶矩和二阶矩估计。

   β1 系数为指数衰减率，控制权重分配（动量与当前梯度），通常取接近于1的值，默认为0.9。β2 系数为指数衰减率，控制之前的梯度平方的影响情况。

   一阶矩估计 $m_t$ 帮助控制参数更新的方向，以确保在梯度的平均方向上前进，从而加速收敛。而二阶矩估计 $v_t$ 帮助控制步长的大小，以确保在不同方向上梯度的平方平均值适当，不至于步子太大或太小。

3. 矫正偏差（Bias Correction）

   Adam算法使用指数移动平均来计算 $m_t$ 和$ v_t$。在初始迭代阶段，由于它们的初始值接近于0，指数移动平均的效果会受到影响，导致它们的值在开始时变化较大。为了解决这个问题，Adam算法引入了偏差校正（bias correction）。偏差校正是通过除以一个校正	因子来调整 $m_t$ 和 $v_t$，以消除在初始迭代阶段的偏差。校正因子是与迭代次数 t 有关的，它会随着迭代的进行逐渐减小，$\hat{m_t}$ 和$\hat{v_t}$ 在初始阶段更稳定。

$$
\hat{v_t} = v_t / (1 - β_2^t)
$$

$$
\hat{m_t} = m_t / (1 - β_1^t)
$$

4. 更新参数
   $$
   \theta_{t+1} = \theta_t - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}
   $$
   ε=10^-8，避免除数变为0。
=======
# Iris
>>>>>>> dev
